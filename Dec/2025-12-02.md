# Fine-Tuning과 RAG 차이
    - Fine-Tuning : 모델 자체의 파라미터를 업데이트해서 특정 도메인·작업에 더 잘 맞도록 모델을 재학습하는 방식
    - RAG : 모델은 그대로 두고, 외부 지식을 검색해 LLM에 추가로 넣어 답변을 강화하는 방식(지식만 업데이트, 모델은 고정)

# PEFT

Parameter-Efficient Fine-Tuning : 필요한 파라미터만 업데이트하는 효율적인 미세 조정

# PEFT techniques 
    1. Adapters 
        - 각 Transformer Layer마다 작은 FFN 모듈을 끼워 넣어 조정
    2. LoRA(Low-Rank Adaptation)
        - 원래 weight W에 작은 low-rank 행렬 A, B를 추가해 업데이트
        - W 업데이트 안함 → A, B만 훈련
    3. QLoRA(Quantized LoRA)
        - 모델 본체를 4비트로 압축해서 GPU 메모리 사용을 극한으로 줄이고, 그 위에 LoRA 모듈만 학습
